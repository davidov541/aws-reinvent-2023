Spark for EKS
	~60% of data teams using K8s? Study from 2022 from Data on K8S Group
	Data on EKS (DoEKS) OSS project from AWS
		Reference Architectures
		IaC Templates
		Best Practices
		Sample Test Code
	Internal Custom Terraform Providers
		Example: https://registry.terraform.io/modules/terraform-aws-modules/iam/aws/latest
		Hooks into GitHub/Azure DevOps repo? Needs more investigation
		This method also appears to have some simplified ways of setting up resources: https://registry.terraform.io/modules/terraform-aws-modules/rds/aws/latest
	Using AWS Cloud9 (or other Cloud IDE) may help solve some hesitancy to get into source code
	There are some tools like KubeCost and Grafana that are easy to set up to monitor EKS clusters.
		A good amount of configuration required, but using boilerplate may work here
	There is a gitSync tag in the airflow YAML file which allows for synchronization automatically between a git repo and Airflow. This can be used to deploy DAGs to Airflow, but would it work for multiple teams to one instance?
		Sounds like no. The answer they propose is a root repo with submodules that pull from other repos: https://github.com/apache/airflow/discussions/19381
		Another answer in there is to set up S3 sync and then just have steps to upload the DAGs into a common S3 bucket for use
United Airlines with DocumentDB
	Support for Elastic Clusters (PaaS instead of IaaS)
	Can trigger Lambda functions when DocumentDB records are changed
	Structure of talk is updating new features of product, and then client testimonial at 20 mins in
	Background -> Why -> Architecture -> Challenges/Solutions
	Applications read/write with DocumentDB. 
		MSK topics used between Mainframe and DocumentDB
		Aurora also populated from MSK for order history for analytics
Data Catalog
	Data Governance should include security around data
	Data Catalogs help Data Governance to determine what data is available and where
	Technical Catalog - Think Hive Metastore (Columns, Location, Names, etc)
		AWS Glue Data Catalog is AWS' solution here
	Business Catalog - Enhances metadata (What does a column name mean? etc)
		Amazon DataZone is AWS' solution here
	DataZone also has ability to federate access (request access, with approval process built-in)
	Verdict: DataZone is pretty interesting with its ability to request and provide access to datasets, but it is still not there, and will likely be AWS-specific only unless vendors decide to support.
	No current support for service accounts as consumers, so access is only for ad-hoc queries
		Work with AWS Glue is on the roadmap for this use case.
		There is also an API that vendors can implement in order to be both producers and consumers
Pragmatic Python
	Technologies:
		mkdocs-material
		pytest
		Pydantic
		Powertools for AWS Lambda
	Common stages:
		Project Setup
		Pre-Commit Check
		Pre-PR
		PR Checks
	Serverless Project:
		IaC
		Domain Code
		Tests
		Readiness (Secondary Requirement Work such as Reliability/Performance/Cost Optimization)
	Devs should own all four of these parts all the way through the lifecycle
	Architectural Layers helps split things up
		Example: Handler vs Domain
		This would allow easy unit testing for domain logic, separates it from the plumbing
		May be worth splitting all integration code out of the domain layer into Integration layer
		This can make testing easier, and switching infrastructure decisions (DynamoDB vs Aurora)
	Goal should be to never leave the IDE (using breakpoints against real cloud resources)
		No Mocks!
	Tests
		Unit -> Run Locally
		Integration -> Run Locally & On AWS. Majority of Tests
		E2E -> Run only on AWS
	Integration tests create test data, call Lambda handler without mocks, causing it to talk to real AWS resources
	What if we had a sandbox environment in Snowflake and a team was using Spark? Could we do something similar, talking to their Sandbox environment from their local environment???
	Stubber is a resource from botocore that allows mocking failures in AWS resources
		Doing this as integration test allows you to only mock what you need
	E2E tests should also include some unhappy paths (excplitly invalid authentication or authorization)
	For Lambda functions, you can add additional parameters with defaults to do dependency injection
	When receiving messages, it's useful to wrap model in a class that holds metadata such as source, timestamp, etc.
	You can use pytest_socket to disable all network connections from tests, to make sure you've got reliable unit tests
	To do E2E testing for async functions, force side effects you can monitor for.
		You can have a step function that gets the result and writes it to a DynamoDB table you can modify
		Make sure that the event metadata allows you to verify it is the result you're looking for.
		Use request.node.name to get name of the current test, and use that as the source so it'll trace through
	Powertools for AWS Lambda has a lot of libraries that can make Lambda jobs easier to write
	Tuna helps visualize stack traces for import time increases, from the Python output
	PySpy also does this for full executions
	PyInstrument tells you how long a specific area of code takes
	Source code and details on decisions made available here: https://github.com/ran-isenberg/serverless-python-demo
Expo
	DBT
		DBT Cloud adds a lot of stability and enforces the development process a bit better than DBT Core
		Theres a book out there on how to use DBT I should read
	Soda
		Competitor to GE
		Managed and OSS versions.
		Often seen as simpler to use than GE
		REST API can be used to trigger DQ checks in managed version
	Astronomer
		14 day free trial now available
AWS Amplify
	Amplify consists of DataSync using GraphQL and Authentication (through Cognito, Auth0, and Okta)
	Focuses on logic, not on infrastructure
	Amplify Gen2 released in preview last week
		Now built entirely on top of CDK
		This allows you to drill down for customization if necessary
		Completely done in TypeScript
		Now able to use a local sandbox for testing
		1:1 mapping between branches and shared environments
		Allows for ephemeral environments for QA
		Able to take a model and generate React code to do CRUD work on that model
	Frontend has access to typed versions from the backend model
	Steps from basic front-end to full application:
		Use CLI via npm to create boilerplate code
		amplify/auth contains authentication information (Cognito by default)
		amplify/data contains the models for the backend
			Models contain information about authorization and relationships between models
		npx amplify sandbox command allows for creating sandbox environment to deploy to
		amplify/backend.ts allows you to customize the resources that are being created using CDK
			To do this, pull the necessary parts out of the resource struct created, and modify properties as necessary
		Custom stacks can be created through a separate TS file and then hooked into the backend.ts file
		npx generate forms creates ui-components directory based on the models for CRUD operations
	AWS Secrets Manager can be pulled into the code as if they were an environment variable
	Support for many front-ends, like React, Angular, native mobile front-ends, etc
		Forms with script generator is only available for React
Aurora Limitless
	Reference tables and sharded tables
	Allows for forcing colocation of tables based on having the same keys (for joins)
	Uses SET commands to control sharding and table type (to avoid changing syntax)
	Internal sub-shards used to move data when a new shard is needed
	You can have standard tables alongside sharded tables
	Internal routers that can scale and shard computes, both of which can scale horizontally or vertically
	Official recommendation is to use serverless unless you need write scalability since you are hitting the limits on writes
	Separate endpoint for sharded tables, which can also be used for non-sharded tables
	There is some way to tag databases as sharded? Not clear on this
	Shards can split if necessary automatically
	Eventually, there will be a path to take existing tables and add them into sharding
SageMaker
	S3 -> AWS Glue Crawler -> AWS Glue Data Catalog -> Athena -> SageMaker
	Data Wrangler or PyAthena to get data from Athena
	